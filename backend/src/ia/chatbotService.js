import { askLLM } from "./llmClient.js";
import { SYSTEM_PROMPT } from "./promptTemplates.js";
import { routeIntent } from "./intentRouter.js";

function cleanJSON(text) {
  return text
    .replace(/```json/gi, "")
    .replace(/```/g, "")
    .trim();
}


/**
 * 1. Detecta intenciÃ³n con Llama (JSON obligatorio)
 */
async function detectIntent(message, adminId, orgId) {
  const response = await askLLM([
    {
      role: "system",
      content: SYSTEM_PROMPT + `
IMPORTANTE:
Debes responder SIEMPRE con un JSON estricto:
{
  "intent": "nombre_del_intent",
  "params": { ... }
}
NO escribas texto fuera del JSON.
NO escribas explicaciones.
`
        },
        { role: "user", content: message }
    ]);

    const content = response.choices[0].message.content;
    console.log("content bruto:", content);

    try {
        const cleaned = cleanJSON(content);

        console.log("content cleaned:", cleaned);

        const parsed = JSON.parse(cleaned);

        // Insertamos adminId y orgId del backend, ignoramos lo que devuelva la IA.
        parsed.params.adminId = adminId;
        parsed.params.orgId = orgId;

        return { success: true, intent: parsed.intent, params: parsed.params };

    } catch (err) {
        return { success: false, message: "La IA no devolviÃ³ un JSON vÃ¡lido.", raw: content };
    }
}

/**
 * Genera un mensaje natural usando los datos reales devueltos por routeIntent.
 * La IA NO puede inventar nada porque solo usa los datos que le enviamos aquÃ­.
 */
async function generateNaturalResponse(intent, params, data) {
  const prompt = `
Eres un asistente Ãºtil de CabinApp.
Debes generar una respuesta clara, amable, en ESPAÃ‘OL,
basada SOLO en los datos proporcionados a continuaciÃ³n.

Nunca inventes nombres, fechas o datos que no estÃ©n en la secciÃ³n "DATOS".
Si no hay datos suficientes, explica quÃ© falta.

---
INTENT: ${intent}

PARAMS:
${JSON.stringify(params, null, 2)}

DATOS (respuesta del backend):
${JSON.stringify(data, null, 2)}
---

Genera una explicaciÃ³n natural, breve y clara para el usuario.
Puedes usar emojis si ayudan, pero no abuses.
`;

  if (intent === "small_talk") {
    const text = params?.originalMessage?.toLowerCase() ?? "";

    if (text.includes("gracias") || text.includes("thank")) {
      return "Â¡Con gusto! ğŸ˜Š Â¿Necesitas algo mÃ¡s?";
    }

    if (text.includes("quien eres") || text.includes("who are you")) {
      return "Soy el asistente inteligente de CabinApp ğŸ¤–. Puedo ayudarte con reservas, clientes y cabaÃ±as. Â¿QuÃ© necesitas?";
    }

    if (text.includes("que puedes hacer") || text.includes("what can you do")) {
      return "Puedo ayudarte a consultar disponibilidad de cabaÃ±as, revisar reservas, listar clientes y responder preguntas del sistema. Â¿QuÃ© deseas hacer?";
    }

    if (text.includes("buenos dÃ­as")) {
      return "Â¡Buenos dÃ­as! â˜€ï¸ Â¿En quÃ© puedo ayudarte hoy?";
    }

    if (text.includes("buenas noches")) {
      return "Â¡Buenas noches! ğŸŒ™ Â¿Necesitas revisar alguna reserva o cabaÃ±a?";
    }

    // respuesta general
    return "Â¡Hola! ğŸ˜Š Â¿En quÃ© puedo ayudarte hoy?";
  }

  const response = await askLLM([
    { role: "system", content: "Eres un generador de respuestas para CabinApp. Solo usa los datos dados." },
    { role: "user", content: prompt }
  ]);

  return response.choices[0].message.content;
}


/**
 * Chatbot principal:
 * Detecta intenciÃ³n â†’ Ejecuta lÃ³gica â†’ Genera respuesta bonita
 */
export async function chatbotHandleMessage(message, adminId, orgId) {

  // 1. Detectamos la intenciÃ³n
  const detection = await detectIntent(message, adminId, orgId);

  if (!detection.success) {
    return detection;
  }

  // 2. Ejecutamos la lÃ³gica real
  const rawData = await routeIntent(detection.intent, detection.params);

  // 3. Generamos una respuesta natural
  const natural = await generateNaturalResponse(
    detection.intent,
    detection.params,
    rawData
  );

  return {
    intent: detection.intent,
    params: detection.params,
    data: rawData,
    respuesta: natural
  };
}